{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TDMSundriyal/Projects/blob/main/Behavioural_Analysis/Mini_Project_2_(Analysis_of_Behavioural_Data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Files"
      ],
      "metadata": {
        "id": "RP5Tzaz7Ftr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAspFBPFgD_",
        "outputId": "56ba2c90-ea01-4328-d453-159e09e7f0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#for dataset importing and visualization\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "## for cleaning purpose\n",
        "import re \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data PreProcessing"
      ],
      "metadata": {
        "id": "5kyk0BLVGuyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing dataset"
      ],
      "metadata": {
        "id": "9VsZmaaiHQ8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## starting with importing the data in collab using pandas\n",
        "dataset = pd.read_csv('history.csv')\n",
        "## before we split the dataset we will perform cleaning of the data \n",
        "## because there are strings in this data and ML model cannot take string directly"
      ],
      "metadata": {
        "id": "7w9_1BpWGyug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doqbGLmi9YDM",
        "outputId": "8d71a53d-bd2c-47cc-90d2-b3c7fb2869f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  urls  \\\n",
            "0    https://www.google.com/search?q=how+do+I+store...   \n",
            "1    https://rapid-cloud.ru/embed-6/46Gdl7QBp0fZ?z=...   \n",
            "2    https://zoro.to/watch/beyblade-burst-gachi-792...   \n",
            "3      https://zoro.to/watch/beyblade-burst-gachi-7929   \n",
            "4    https://zoro.to/beyblade-burst-gachi-7929?ref=...   \n",
            "..                                                 ...   \n",
            "119  https://www.google.com/search?q=google+.com&oq...   \n",
            "120  https://www.google.com/search?q=google+.com&oq...   \n",
            "121  https://www.google.com/search?q=google+.com&oq...   \n",
            "122  https://www.google.com/search?q=google+.com&oq...   \n",
            "123  https://www.google.com/search?q=google+.com&oq...   \n",
            "\n",
            "                                                 texts           dates  \\\n",
            "0    how do I store my history in a csv file using ...  7/8/2022 22:30   \n",
            "1                                                cloud  6/15/2022 9:35   \n",
            "2    Watch Beyblade Burst Gachi English Sub/Dub onl...  6/15/2022 9:35   \n",
            "3    Watch Beyblade Burst Gachi English Sub/Dub onl...  6/15/2022 9:35   \n",
            "4    Watch Beyblade Burst Gachi English Sub/Dub onl...  6/15/2022 9:35   \n",
            "..                                                 ...             ...   \n",
            "119  Predicting Human Behaviour Activity using Deep...  9/13/2022 8:37   \n",
            "120  behaviour analysis using machine learning - Go...  9/14/2022 8:37   \n",
            "121  behaviour analysis using machine learning - Go...  9/15/2022 8:37   \n",
            "122  behaviour analysis using machine learning - Go...  9/16/2022 8:37   \n",
            "123  Unstop - Competitions, Quizzes, Hackathons, Sc...  9/17/2022 8:37   \n",
            "\n",
            "          Interest  \n",
            "0            study  \n",
            "1            study  \n",
            "2           Tharki  \n",
            "3    entertainment  \n",
            "4    entertainment  \n",
            "..             ...  \n",
            "119          study  \n",
            "120          study  \n",
            "121          study  \n",
            "122          study  \n",
            "123          study  \n",
            "\n",
            "[124 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning of the data "
      ],
      "metadata": {
        "id": "_D3sC3V7HU75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this step we will clean the data by removing any kind of punctuations, special symbols basically everything except alphabets. \n",
        "### Once its done, we will then add these into the lists called cleaned_data and this list will furthur be used to create the matrix of features."
      ],
      "metadata": {
        "id": "NygY3o7PHln1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## we will create an empty list where we will store the cleaned text\n",
        "cleaned_data = [] \n",
        "for i in range(0, len(dataset)):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', str(dataset['texts'][i]))\n",
        "  # review = str(dataset['texts'][i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  #Stemming the data\n",
        "  stemmer = PorterStemmer()\n",
        "  review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ' '.join(review)\n",
        "  cleaned_data.append(review)"
      ],
      "metadata": {
        "id": "DthID0i3HYf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## printing the cleaned words line by line\n",
        "for item in cleaned_data:\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tg0ym0xJB_F",
        "outputId": "23cc18d5-a1dc-47ac-ce47-2521ba41d281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "store histori csv file use python program googl search\n",
            "cloud\n",
            "watch beyblad burst gachi english sub dub onlin free zoro\n",
            "watch beyblad burst gachi english sub dub onlin free zoro\n",
            "watch beyblad burst gachi english sub dub onlin free zoro\n",
            "watch anim onlin free anim stream onlin zoro anim websit\n",
            "watch anim onlin free anim stream onlin zoro anim websit\n",
            "zoro tv googl search\n",
            "rapid\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "rapid cloud\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "rapid cloud\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "rapid cloud\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "rapid cloud\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "watch beyblad burst spark ona episod english sub dub onlin free zoro\n",
            "watch anim onlin free anim stream onlin zoro anim websit\n",
            "rapid cloud\n",
            "rapid cloud\n",
            "watch beyblad g revolut english sub dub onlin free zoro\n",
            "watch beyblad g revolut english sub dub onlin free zoro\n",
            "watch beyblad g revolut english sub dub onlin free zoro\n",
            "watch anim onlin free anim stream onlin zoro anim websit\n",
            "zoro tv googl search\n",
            "googl\n",
            "googl com googl search\n",
            "regress analysi machin learn\n",
            "machin learn regress\n",
            "instagram\n",
            "search algorithm\n",
            "geek geek\n",
            "gforg\n",
            "depth first search\n",
            "df algorithm\n",
            "graphic era univers\n",
            "ye googl search\n",
            "coursera\n",
            "faq errata coursera\n",
            "qualifi quiz\n",
            "suppos compil\n",
            "softwar secur\n",
            "php\n",
            "\n",
            "\n",
            "reddit\n",
            "javapoint\n",
            "export chrome histori chrome web store\n",
            "export chrome histori chrome web store\n",
            "extract histori chrome googl search\n",
            "extract histori chrome googl search\n",
            "extract histori chrome googl search\n",
            "csv file open\n",
            "timeandd com\n",
            "timeandd com\n",
            "ms excel import live data web excel youtub\n",
            "timeandd com\n",
            "self learn\n",
            "csv file open\n",
            "csv file open\n",
            "ms excel import live data web excel youtub\n",
            "csv file googl search\n",
            "csv file googl search\n",
            "csv file googl search\n",
            "learn basic machin learn free ml beginn program\n",
            "make csv file youtub\n",
            "creat csv file youtub\n",
            "download instal browser histori examin rc youtub\n",
            "learn basic machin learn free ml beginn program\n",
            "free onlin cours power futur skillup simplilearn\n",
            "predict human behaviour activ use deep learn lstm ajay kumar sharma medium\n",
            "download instal browser histori examin rc youtub\n",
            "download brower histori packag youtub\n",
            "self learn\n",
            "uttaranch univers campu tour episod youtub\n",
            "walk uttaranch univers youtub\n",
            "uttaranch univers campu tour episod youtub\n",
            "walk uttaranch univers youtub\n",
            "walk uttaranch univers youtub\n",
            "uttaranch univers googl search\n",
            "uttaranch univers googl search\n",
            "uttaranch univers googl search\n",
            "uttaranch univers googl search\n",
            "uttaranch univers googl search\n",
            "behaviour analysi use machin learn googl search\n",
            "self learn\n",
            "learn basic machin learn free ml beginn program\n",
            "self learn\n",
            "dashboard\n",
            "learn simplilearn\n",
            "learn simplilearn\n",
            "learn simplilearn\n",
            "dashboard\n",
            "learn simplilearn\n",
            "learn simplilearn\n",
            "learn simplilearn\n",
            "skillup free cours googl search\n",
            "free onlin cours power futur skillup simplilearn\n",
            "learn basic machin learn free ml beginn program\n",
            "introduct deep learn\n",
            "introduct deep learn\n",
            "free onlin cours power futur skillup simplilearn\n",
            "free onlin cours power futur skillup simplilearn\n",
            "free onlin cours power futur skillup simplilearn\n",
            "free onlin cours power futur skillup simplilearn\n",
            "onlin cours certif program profession skillup onlin\n",
            "skillup free cours googl search\n",
            "skillup free cours googl search\n",
            "skillup free cours googl search\n",
            "free pdf editor free download window\n",
            "download free pdf editor window free\n",
            "pdf editor download pc googl search\n",
            "pdf editor download pc googl search\n",
            "pdf editor download pc googl search\n",
            "predict human behaviour activ use deep learn lstm ajay kumar sharma medium\n",
            "predict human behaviour activ use deep learn lstm ajay kumar sharma medium\n",
            "predict human behaviour activ use deep learn lstm ajay kumar sharma medium\n",
            "behaviour analysi use machin learn googl search\n",
            "behaviour analysi use machin learn googl search\n",
            "behaviour analysi use machin learn googl search\n",
            "unstop competit quizz hackathon scholarship internship student corpor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Matrix of Features"
      ],
      "metadata": {
        "id": "KfHLkz5jJpD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MOF ## dependent variables"
      ],
      "metadata": {
        "id": "AdiW09cR-vOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we will create matrix of feautures from cleaned_data by converting the string list\n",
        "## into 2-D matrix called as Matrix of Features by using the technique called vectorization.\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "x = cv.fit_transform(cleaned_data).toarray()\n",
        "print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKo4e3L0JoaX",
        "outputId": "c22acf87-1b64-4fb1-abcb-ae7c2a1debe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Categorical Data"
      ],
      "metadata": {
        "id": "3oFaQGP_M60u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset.iloc[:, -1] ## last column will be taken which consists of the interests\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "bNbsvlmZM-sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vySwsjO8PpxF",
        "outputId": "02b807dc-6b1f-4547-bfa3-421059a89623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 2 2 2 2\n",
            " 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2\n",
            " 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data"
      ],
      "metadata": {
        "id": "_tw1CAkxrEZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
      ],
      "metadata": {
        "id": "p9e_eEHCrJuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model "
      ],
      "metadata": {
        "id": "Y6PfVwXoqbS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "NIwbfD6DBMh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier1 = GaussianNB()\n",
        "classifier1.fit(X_train, y_train) ## here we are training the model so that we can use it later on"
      ],
      "metadata": {
        "id": "u9TsPWIrqgTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37927400-e1ad-441c-e14f-975c6a39b29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HtufDiQ4CCPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistics Regression"
      ],
      "metadata": {
        "id": "GelgCJ-dQLGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier2 = LogisticRegression(random_state = 0)\n",
        "classifier2.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko9Wl3O1QPKN",
        "outputId": "243bab37-d33a-478d-bafd-1ebb281c4275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "ppOmEADLxrGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier3 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "classifier3.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYFlAMS6xqqG",
        "outputId": "790aee86-c1af-466b-976b-015fac402d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONFUSION MATRIX"
      ],
      "metadata": {
        "id": "iEGX9XvbrRuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier1.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "LzRaqSxErZMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfe9c20-02ba-496f-9568-0faba4356964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing against Custom Input"
      ],
      "metadata": {
        "id": "TSQiIL9QQVG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1 - Study and 0 - Entertainment\n",
        "string = input(\"Enter a string-->\",)\n",
        "new = []\n",
        "new.append(string)\n",
        "new = cv.transform(new).toarray()\n",
        "custom_predict = classifier1.predict(new)\n",
        "if(custom_predict == 0):\n",
        "  print(\"User is Interested in Entertainment and fun\")\n",
        "else:\n",
        "  print(\"User is interested in Education related content\")"
      ],
      "metadata": {
        "id": "2cXB4mfXQY1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}